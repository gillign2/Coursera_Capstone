{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Notebook & Github Setup"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# this notebook will be mainly used for the Coursera capstone project\n\nimport pandas as pd\nimport numpy as np\n\nprint(\"Hello Capstone Project Course!\")",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Hello Capstone Project Course!\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Predicting Seattle Car Accident Severity\n### by: Christina Gilligan"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Business Problem & Understanding\n\nThe objective of this project is to understand:\n\n**IF** there is a way to predict the possibility of getting a car accident and how severe the accident would be based on factors such as road conditions, weather, and visibility.  \n**SO THAT WE** we can use this information to warn and inform drivers so that they can drive more carefully or adjust their travel plans if necessary\n\nTo do so we will be utilizing the dataset 'Data-Collisions.csv' which details all collisions provided by the Seattle Police Department (SPD) and recorded by Traffic Records from 2004-present. From this dataset we will train and implement different machine learning models to help:\n1. Predict how the above driving conditions affect the severity of a collision and\n2. Create a 'Collision Rating Scale' to assess the probability of an accident occurring with 1 being the lowest probability of an accident occurring and 5 being the highest\n    \n\n## Data Understanding & Preparation\n\n### Data Understanding\nThis project will be utilizing the shared data set 'Data-Collisions.csv' provided to us in Week 1 of the capstone course. This dataset details all collisions provided by the Seattle Police Department (SPD) and recorded by Traffic Records from 2004-present. The dataset contains 37 different collision attributes which are not all relevant to our modeling purposes. In order to reduce computational costs and prepare the dataset for the models we will drop these attributes that contain irrelevant information. The specific columns/attributes we will be using for this model to predict the accident severity SEVERITYCODE (y/dependent variable) are WEATHER (x1, independent variable), ROADCOND (x2, independent variable), and LIGHTCOND (x3, independent variable) which detail the weather conditions, road conditions (wet, dry, etc.) and light conditions (daylight, nighttime, etc.) that were present during the collision.\n\n**Predictor/Target Variable:** 'SEVERITYCODE'\n\n    1: Property damage\n    2: Property damage and injury\n    \n**Independent Variables:** 'WEATHER', 'ROADCOND', 'LIGHTCOND'\n\n### Data Preparation\nThe native version of the dataset is crowded and not fit for analysis. There are too many columns that we will not need for our models so we will need to drop those and only keep the ones discussed above. Also, the dataset is unbalanced so we will need to transform the target variable (SEVERITYCODE) to balance the data before we can model."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Download the dataset\n!wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "--2020-09-29 15:43:38--  https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv\nResolving s3.us.cloud-object-storage.appdomain.cloud (s3.us.cloud-object-storage.appdomain.cloud)... 67.228.254.196\nConnecting to s3.us.cloud-object-storage.appdomain.cloud (s3.us.cloud-object-storage.appdomain.cloud)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 73917638 (70M) [text/csv]\nSaving to: \u2018Data-Collisions.csv\u2019\n\n100%[======================================>] 73,917,638  48.1MB/s   in 1.5s   \n\n2020-09-29 15:43:40 (48.1 MB/s) - \u2018Data-Collisions.csv\u2019 saved [73917638/73917638]\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Drop irrelevant attributes/columns and set new relevant categories\ndf=pd.read_csv(\"Data-Collisions.csv\")\n\ndasta = df.drop(columns = ['OBJECTID', 'SEVERITYCODE.1', 'REPORTNO', 'INCKEY', 'COLDETKEY', \n                           'X', 'Y', 'STATUS', 'ADDRTYPE', \n                           'INTKEY', 'LOCATION', 'EXCEPTRSNCODE', \n                           'EXCEPTRSNDESC', 'SEVERITYDESC', 'INCDATE', \n                           'INCDTTM', 'JUNCTIONTYPE', 'SDOT_COLCODE', \n                           'SDOT_COLDESC', 'PEDROWNOTGRNT', 'SDOTCOLNUM', \n                           'ST_COLCODE', 'ST_COLDESC', 'SEGLANEKEY', \n                           'CROSSWALKKEY', 'HITPARKEDCAR', 'PEDCOUNT', 'PEDCYLCOUNT', \n                           'PERSONCOUNT', 'VEHCOUNT', 'COLLISIONTYPE', \n                           'SPEEDING', 'UNDERINFL', 'INATTENTIONIND'])\n\ndasta[\"WEATHER\"] = dasta[\"WEATHER\"].astype('category')\ndasta[\"ROADCOND\"] = dasta[\"ROADCOND\"].astype('category')\ndasta[\"LIGHTCOND\"] = dasta[\"LIGHTCOND\"].astype('category')\n\ndasta[\"WEATHER_CAT\"] = dasta[\"WEATHER\"].cat.codes\ndasta[\"ROADCOND_CAT\"] = dasta[\"ROADCOND\"].cat.codes\ndasta[\"LIGHTCOND_CAT\"] = dasta[\"LIGHTCOND\"].cat.codes\n\n# Preview new dataset\ndasta.head(5)",
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 10,
                    "data": {
                        "text/plain": "   SEVERITYCODE   WEATHER ROADCOND                LIGHTCOND  WEATHER_CAT  \\\n0             2  Overcast      Wet                 Daylight            4   \n1             1   Raining      Wet  Dark - Street Lights On            6   \n2             1  Overcast      Dry                 Daylight            4   \n3             1     Clear      Dry                 Daylight            1   \n4             2   Raining      Wet                 Daylight            6   \n\n   ROADCOND_CAT  LIGHTCOND_CAT  \n0             8              5  \n1             8              2  \n2             0              5  \n3             0              5  \n4             8              5  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SEVERITYCODE</th>\n      <th>WEATHER</th>\n      <th>ROADCOND</th>\n      <th>LIGHTCOND</th>\n      <th>WEATHER_CAT</th>\n      <th>ROADCOND_CAT</th>\n      <th>LIGHTCOND_CAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>Overcast</td>\n      <td>Wet</td>\n      <td>Daylight</td>\n      <td>4</td>\n      <td>8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Raining</td>\n      <td>Wet</td>\n      <td>Dark - Street Lights On</td>\n      <td>6</td>\n      <td>8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Overcast</td>\n      <td>Dry</td>\n      <td>Daylight</td>\n      <td>4</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Clear</td>\n      <td>Dry</td>\n      <td>Daylight</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Raining</td>\n      <td>Wet</td>\n      <td>Daylight</td>\n      <td>6</td>\n      <td>8</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Transform the target variable (SEVERITYCODE) to balance the data before we model\n\nfrom sklearn.utils import resample\ndasta_more = dasta[dasta.SEVERITYCODE==1]\ndasta_less = dasta[dasta.SEVERITYCODE==2]\ndasta_more_equal = resample(dasta_more,\n                            replace=False,\n                            n_samples=58188,\n                            random_state=99)\ndasta_bal = pd.concat([dasta_more_equal, dasta_less])\ndasta_bal.SEVERITYCODE.value_counts()",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 14,
                    "data": {
                        "text/plain": "2    58188\n1    58188\nName: SEVERITYCODE, dtype: int64"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Data Modeling & Methodology\n\nNow that we have cleaned up and transformed our data it's time to train the models. We will utilize 3 different models to determine which is the best fit for the problem. The models we will be using are logistic regression, K nearest neighbor, and decision tree."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import preprocessing",
            "execution_count": 20,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Data preprocessing (define X and y, normalize the dataset)\nX = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\nX = np.asarray(dasta_bal[['WEATHER_CAT', 'ROADCOND_CAT', 'LIGHTCOND_CAT']])\nX[0:5]\ny = np.asarray(dasta_bal['SEVERITYCODE'])\ny [0:5]",
            "execution_count": 21,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int8 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int8 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n",
                    "name": "stderr"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 21,
                    "data": {
                        "text/plain": "array([1, 1, 1, 1, 1])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# I'm using an 80% train and 20% test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\nprint('Train set:', X_train.shape, y_train.shape)\nprint('Test set:', X_test.shape, y_test.shape)",
            "execution_count": 57,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train set: (93100, 3) (93100,)\nTest set: (23276, 3) (23276,)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Logistic Regression Model\n\nThe dataset only gives us 2 outcomes for SEVERITYCODE (1,2) which makes it binary which fits wonderfully with Logistic Regression."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix",
            "execution_count": 24,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "logReg = LogisticRegression(C=6, solver='liblinear').fit(X_train,y_train)\nlogReg",
            "execution_count": 66,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 66,
                    "data": {
                        "text/plain": "LogisticRegression(C=6, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n          tol=0.0001, verbose=0, warm_start=False)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "logPred = logReg.predict(X_test)\nlogPred",
            "execution_count": 67,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 67,
                    "data": {
                        "text/plain": "array([2, 2, 2, ..., 1, 2, 2])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "logPredodd = logReg.predict_proba(X_test)\nlogPredodd",
            "execution_count": 68,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 68,
                    "data": {
                        "text/plain": "array([[0.47085214, 0.52914786],\n       [0.47085214, 0.52914786],\n       [0.47085214, 0.52914786],\n       ...,\n       [0.53547329, 0.46452671],\n       [0.47085214, 0.52914786],\n       [0.47085214, 0.52914786]])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### K Nearest Neighbor Model\n\nKNN helps predict the SEVERITYCODE of an outcome by finding similar data points within k distance."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.neighbors import KNeighborsClassifier",
            "execution_count": 32,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "ks = 15\nhood = KNeighborsClassifier(n_neighbors = ks).fit(X_train,y_train)\nhood\nhoodPred = hood.predict(X_test)\nhoodPred[0:5]",
            "execution_count": 33,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 33,
                    "data": {
                        "text/plain": "array([2, 2, 2, 2, 2])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Decision Tree Model\n\nThe Decision Tree will show us all possible outcomes so we can analyze all the consequences of a decision. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.tree import DecisionTreeClassifier",
            "execution_count": 34,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "treedat = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 5)\ntreedat\ntreedat.fit(X_train, y_train)",
            "execution_count": 58,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 58,
                    "data": {
                        "text/plain": "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "DTyhat = treedat.predict(X_test)\nprint (treePred[0:5])\nprint (y_test[0:5])",
            "execution_count": 65,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "[2 2 2 2 2]\n[2 1 1 1 2]\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Model Evaluation & Results\nBefore proceeding to deployment, we will evaluate the three models we trained above to determine which model most accurately meets our business objective. To do so, we will utilize 3 different evaluation metrics: F1 Score, Jaccard Similarity Index and Log Loss (Logistic Regression Model only)."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Import the measurements from sklearn\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import log_loss",
            "execution_count": 39,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Logistic Regression Model Evaluation"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# F1 Score Evaluation\nlrf1 = f1_score(y_test, logPred, average='macro')\nprint('The F1 Score is', lrf1)",
            "execution_count": 43,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "The F1 Score is 0.5149615460549232\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Jaccard Similarity Index Evaluation\nlrjac=jaccard_similarity_score(y_test, logPred)\nprint('The Jaccard Score is', lrjac)",
            "execution_count": 42,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "The Jaccard Score is 0.5293864925244888\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Log Loss Evaluation\nlrlogloss=log_loss(y_test, logPredodd)\nprint('The Log Loss Score is', lrlogloss)",
            "execution_count": 45,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "The Log Loss Score is 0.6839726419666594\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### K Nearest Neighbor Model Evaluation"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# F1 Score Evaluation\nknnf1=f1_score(y_test, hoodPred, average='macro')\nprint('The F1 Score is', knnf1)",
            "execution_count": 53,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "The F1 Score is 0.5431647126727994\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Jaccard Similarity Index Evaluation\nknnjac=jaccard_similarity_score(y_test, hoodPred)\nprint('The Jaccard Score is', knnjac)",
            "execution_count": 52,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "The Jaccard Score is 0.559245574841038\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Decision Tree Model Evaluation"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# F1 Score Evaluation\ntreef1=f1_score(y_test, treePred, average='macro')\nprint('The F1 Score is', treef1)",
            "execution_count": 55,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "The F1 Score is 0.5334047084927266\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Jaccard Similarity Index Evaluation\ntreejac=jaccard_similarity_score(y_test, treePred)\nprint('The Jaccard Score is', treejac)",
            "execution_count": 56,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "The Jaccard Score is 0.5598470527582059\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Discussion\n\nAfter training 3 different machine learning model types (Logistic Regression, KNN and Decision Tree) and then performing the appropriate model evaluations for each of the models it can be determined that the Logistic Regression Model performs with the highest accuracy and therefor is the optimal model for the task. This is likely due to the data's binary nature (severity codes 1 and 2)."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Conclusion\n\nBased on the historical collision data collected by the Seattle Police Department that includes attributes such as weather conditions, road conditions, and light conditions we can reasonably conclude that particular combinations of these factors somewhat have an impac on whether or not an individual's travel could results in property damage and or bodily injury. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}